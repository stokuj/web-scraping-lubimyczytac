# ğŸ“š Web Scraper przy uÅ¼yciu Selenium

Ten projekt to prosty, ale elastyczny web scraper napisany w Pythonie, sÅ‚uÅ¼Ä…cy do zbierania danych z portalu **LubimyczytaÄ‡.pl**. GÅ‚Ã³wnym celem jest automatyczne pobieranie informacji o ksiÄ…Å¼kach (np. tytuÅ‚, autor, ocena, pÃ³Å‚ki uÅ¼ytkownika) z publicznego profilu uÅ¼ytkownika, a nastÄ™pnie zapisanie ich do pliku CSV w celu dalszej analizy.

## ğŸ“‹ Spis treÅ›ci
- [ObsÅ‚ugiwane strony](#obsÅ‚ugiwane-strony)
- [Technologie uÅ¼yte w projekcie](#-technologie-uÅ¼yte-w-projekcie)
- [Struktura projektu](#-struktura-projektu)
- [Instalacja](#-instalacja)
- [Konfiguracja](#-konfiguracja)
- [UÅ¼ycie](#-uÅ¼ycie)
- [FunkcjonalnoÅ›ci](#-funkcjonalnoÅ›ci)
- [Testowanie](#-testowanie)
- [MoÅ¼liwe rozszerzenia](#-moÅ¼liwe-rozszerzenia)

## ObsÅ‚ugiwane strony

- **Goodreads** â€” Radzi sobiÄ™ z wiÄ™kszoÅ›ciÄ… importem wiÄ™kszoÅ›ci tyuÅ‚Ã³w

## ğŸ”§ Technologie uÅ¼yte w projekcie

- **Python 3**
- **Selenium** â€” automatyzacja przeglÄ…darki (Chrome)
- **ChromeDriver** â€” sterowanie przeglÄ…darkÄ… Chrome

## ğŸ“ Struktura projektu

```
/web_scraping_lubimyczytac/
â”‚
â”œâ”€â”€ main.py                  # GÅ‚Ã³wna aplikacja â€” punkt wejÅ›cia, uruchamia scrapowanie i zapis
â”œâ”€â”€ scrapper.py              # Logika pobierania danych ze strony (Selenium)
â”œâ”€â”€ table_utils.py           # Przetwarzanie i zapis danych do CSV
â”œâ”€â”€ dane/                    # Katalog na wygenerowane dane
â”‚   â””â”€â”€ books.csv            # Plik wynikowy z danymi ksiÄ…Å¼ek
â””â”€â”€ requirements.txt         # Lista wymaganych bibliotek
```

## âœ… FunkcjonalnoÅ›ci

- ObsÅ‚uga paginacji (przechodzi przez wszystkie strony uÅ¼ytkownika)
- Pobieranie szczegÃ³Å‚owych danych o ksiÄ…Å¼kach:
  - TytuÅ‚, autor, cykl
  - Åšrednia ocena, ocena uÅ¼ytkownika
  - Liczba ocen, czytelnikÃ³w, opinii
  - PÃ³Å‚ki (`shelves` i `self_shelfs`)
  - Data przeczytania (jeÅ›li dostÄ™pna)
- ObsÅ‚uga wyjÄ…tkÃ³w i bÅ‚Ä™dÃ³w (np. brak danych na stronie, timeouty)
- Zapis danych do formatu CSV

## ğŸ§ª Testowanie

Projekt zawiera testy jednostkowe, ktÃ³re sprawdzajÄ… poprawnoÅ›Ä‡ dziaÅ‚ania gÅ‚Ã³wnych funkcji:

- **test_table_utils.py** - testy dla funkcji przetwarzania i zapisu danych
- **test_scrapper.py** - testy dla funkcji pobierania danych ze strony
- **test_sample.py** - podstawowe testy weryfikujÄ…ce dziaÅ‚anie projektu

### Uruchamianie testÃ³w

Aby uruchomiÄ‡ wszystkie testy, uÅ¼yj skryptu `run_tests.py`:

```
python run_tests.py
```

MoÅ¼esz rÃ³wnieÅ¼ uruchomiÄ‡ poszczegÃ³lne testy za pomocÄ… pytest:

```
pytest -v tests/test_table_utils.py
pytest -v tests/test_scrapper.py
pytest -v tests/test_sample.py
```

### Dodawanie nowych testÃ³w

Aby dodaÄ‡ nowe testy, utwÃ³rz nowy plik w katalogu `tests/` z nazwÄ… zaczynajÄ…cÄ… siÄ™ od `test_`. Funkcje testowe powinny rÃ³wnieÅ¼ zaczynaÄ‡ siÄ™ od `test_`.

## ğŸ“¥ Instalacja

1. Sklonuj repozytorium:
   ```
   git clone https://github.com/twoj-username/web_scraping_lubimyczytac.git
   cd web_scraping_lubimyczytac
   ```

2. Zainstaluj wymagane biblioteki:
   ```
   pip install -r requirements.txt
   ```

3. Pobierz i zainstaluj ChromeDriver:
   - Pobierz odpowiedniÄ… wersjÄ™ [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/downloads) dla Twojej wersji przeglÄ…darki Chrome
   - Rozpakuj plik i umieÅ›Ä‡ go w Å›cieÅ¼ce systemowej lub w katalogu projektu

## âš™ï¸ Konfiguracja

1. OtwÃ³rz plik `config.ini` i ustaw URL profilu uÅ¼ytkownika, ktÃ³rego ksiÄ…Å¼ki chcesz pobraÄ‡:
   ```ini
   [settings]
   profile_url = https://lubimyczytac.pl/profil/TWOJ_PROFIL/
   ```

2. Dostosuj parametry w pliku `main.py` wedÅ‚ug potrzeb:
   - Odkomentuj odpowiednie sekcje kodu, aby wÅ‚Ä…czyÄ‡ lub wyÅ‚Ä…czyÄ‡ poszczegÃ³lne etapy przetwarzania
   - MoÅ¼esz zmieniÄ‡ nazwy plikÃ³w wyjÅ›ciowych

## ğŸš€ UÅ¼ycie

1. Uruchom skrypt gÅ‚Ã³wny:
   ```
   python main.py
   ```

2. Skrypt wykona nastÄ™pujÄ…ce operacje (w zaleÅ¼noÅ›ci od odkomentowanych sekcji):
   - Pobranie danych o ksiÄ…Å¼kach z profilu uÅ¼ytkownika
   - Zapisanie danych do pliku CSV
   - Wzbogacenie danych o ISBN i oryginalne tytuÅ‚y
   - Konwersja danych do formatu Goodreads

3. Wyniki zostanÄ… zapisane w katalogu `dane/`:
   - `books.csv` - podstawowe dane o ksiÄ…Å¼kach
   - `books_enriched.csv` - dane wzbogacone o ISBN i oryginalne tytuÅ‚y
   - `goodreads.csv` - dane w formacie gotowym do importu do Goodreads

## ğŸ§  MoÅ¼liwe rozszerzenia

- UÅ¼ycie SQLite lub pandas do dalszej analizy
- Eksport do Excela lub JSON
- GUI (np. przy uÅ¼yciu Tkintera lub PyWebIO)
- Automatyczne logowanie (jeÅ›li wymagane dla prywatnych danych)
- ObsÅ‚uga wielu uÅ¼ytkownikÃ³w jednoczeÅ›nie
- Generowanie statystyk i wykresÃ³w na podstawie zebranych danych
- Integracja z API innych serwisÃ³w ksiÄ…Å¼kowych

# ğŸ“š Web Scraper using Selenium

This project is a simple yet flexible web scraper written in Python, designed to collect data from the **Lubimyczytac.pl** portal. The main goal is to automatically retrieve information about books (e.g., title, author, rating, user shelves) from a user's public profile, and then save them to a CSV file for further analysis.

## ğŸ“‹ Table of Contents
- [Supported Sites](#supported-sites)
- [Technologies Used](#-technologies-used)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [Features](#-features)
- [Testing](#-testing-1)
- [Possible Extensions](#-possible-extensions)

## Supported Sites

- **Goodreads** â€” Handles the import of most titles

## ğŸ”§ Technologies Used

- **Python 3**
- **Selenium** â€” browser automation (Chrome)
- **ChromeDriver** â€” Chrome browser control

## ğŸ“ Project Structure

```
/web_scraping_lubimyczytac/
â”‚
â”œâ”€â”€ main.py                  # Main application â€” entry point, runs scraping and saving
â”œâ”€â”€ scrapper.py              # Logic for retrieving data from the site (Selenium)
â”œâ”€â”€ table_utils.py           # Processing and saving data to CSV
â”œâ”€â”€ dane/                    # Directory for generated data
â”‚   â””â”€â”€ books.csv            # Output file with book data
â””â”€â”€ requirements.txt         # List of required libraries
```

## âœ… Features

- Pagination handling (navigates through all user pages)
- Retrieving detailed book data:
  - Title, author, series
  - Average rating, user rating
  - Number of ratings, readers, reviews
  - Shelves (`shelves` and `self_shelfs`)
  - Date read (if available)
- Exception and error handling (e.g., missing data on the page, timeouts)
- Saving data to CSV format

## ğŸ§ª Testing

The project includes unit tests that verify the correctness of the main functions:

- **test_table_utils.py** - tests for data processing and saving functions
- **test_scrapper.py** - tests for web scraping functions
- **test_sample.py** - basic tests verifying project functionality

### Running Tests

To run all tests, use the `run_tests.py` script:

```
python run_tests.py
```

You can also run individual tests using pytest:

```
pytest -v tests/test_table_utils.py
pytest -v tests/test_scrapper.py
pytest -v tests/test_sample.py
```

### Adding New Tests

To add new tests, create a new file in the `tests/` directory with a name starting with `test_`. Test functions should also start with `test_`.

## ğŸ“¥ Installation

1. Clone the repository:
   ```
   git clone https://github.com/your-username/web_scraping_lubimyczytac.git
   cd web_scraping_lubimyczytac
   ```

2. Install required libraries:
   ```
   pip install -r requirements.txt
   ```

3. Download and install ChromeDriver:
   - Download the appropriate version of [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/downloads) for your Chrome browser version
   - Extract the file and place it in your system path or in the project directory

## âš™ï¸ Configuration

1. Open the `config.ini` file and set the URL of the user profile you want to scrape:
   ```ini
   [settings]
   profile_url = https://lubimyczytac.pl/profil/YOUR_PROFILE/
   ```

2. Adjust parameters in the `main.py` file as needed:
   - Uncomment the appropriate code sections to enable or disable specific processing stages
   - You can change the output filenames

## ğŸš€ Usage

1. Run the main script:
   ```
   python main.py
   ```

2. The script will perform the following operations (depending on which sections are uncommented):
   - Scrape book data from the user's profile
   - Save the data to a CSV file
   - Enrich the data with ISBN and original titles
   - Convert the data to Goodreads format

3. Results will be saved in the `dane/` directory:
   - `books.csv` - basic book data
   - `books_enriched.csv` - data enriched with ISBN and original titles
   - `goodreads.csv` - data in a format ready for import into Goodreads

## ğŸ§  Possible Extensions

- Using SQLite or pandas for further analysis
- Export to Excel or JSON
- GUI (e.g., using Tkinter or PyWebIO)
- Automatic login (if required for private data)
- Handling multiple users simultaneously
- Generating statistics and charts based on collected data
- Integration with APIs of other book services
